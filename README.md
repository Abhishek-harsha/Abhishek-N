# Abhishek-N
# ğŸ§  Task-02: Image Generation with Pre-trained Models

### ğŸ“ Internship at Prodigy Infotech  
**Domain:** AI / Machine Learning  
**Task:** Generate images from text prompts using pre-trained generative models.

---

## ğŸš€ Objective

To explore and implement **text-to-image generation** using cutting-edge **pre-trained models** like **Stable Diffusion** and **DALLÂ·E-mini**, transforming natural language descriptions into visual outputs.

---

## ğŸ§° Tools & Technologies

- ğŸ Python
- ğŸ¤— Hugging Face `diffusers`
- ğŸ”¥ PyTorch
- ğŸ§  Stable Diffusion v1.4 (`CompVis`)
- ğŸ–¼ï¸ DALLÂ·E-mini / Craiyon (optional)

---

## ğŸ§ª Experiment Setup

- Used Hugging Faceâ€™s `diffusers` to access `Stable Diffusion v1-4`.
- Loaded model to `cuda` or `cpu` based on availability.
- Prompt used for generation:
  

- Generated and saved image output as `futuristic_city.png`.

---

## ğŸ“¸ Sample Output

(./futuristic_city.png)
C:\Users\a<img width="512" height="512" alt="futuristic_city (2)" src="https://github.com/user-attachments/assets/ecf2c2cc-a0f1-44db-9909-e51f2a8829f6" />
bhi\Downloads\futuristic_city (2).png

> *Image generated using Stable Diffusion based on the text prompt above.*

---

## ğŸ“Œ Key Learnings

- Understanding latent diffusion techniques for image generation.
- Prompt engineering â€“ how prompt detail impacts image fidelity.
- Comparing generation quality between DALLÂ·E-mini and Stable Diffusion.

---

## ğŸ’¡ How to Run the Code

### ğŸ”— [Google Colab Notebook](https://colab.research.google.com/drive/1f4X8hrYkroKXWjO7zWqbywSxPujz6eh1?usp=sharing)

```bash
# 1. Clone this repository or open the notebook
# 2. Run the cells step by step
# 3. Provide your Hugging Face token when prompted
# 4. Customize your own text prompts
