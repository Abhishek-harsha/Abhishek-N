# Abhishek-N
# 🧠 Task-02: Image Generation with Pre-trained Models

### 📍 Internship at Prodigy Infotech  
**Domain:** AI / Machine Learning  
**Task:** Generate images from text prompts using pre-trained generative models.

---

## 🚀 Objective

To explore and implement **text-to-image generation** using cutting-edge **pre-trained models** like **Stable Diffusion** and **DALL·E-mini**, transforming natural language descriptions into visual outputs.

---

## 🧰 Tools & Technologies

- 🐍 Python
- 🤗 Hugging Face `diffusers`
- 🔥 PyTorch
- 🧠 Stable Diffusion v1.4 (`CompVis`)
- 🖼️ DALL·E-mini / Craiyon (optional)

---

## 🧪 Experiment Setup

- Used Hugging Face’s `diffusers` to access `Stable Diffusion v1-4`.
- Loaded model to `cuda` or `cpu` based on availability.
- Prompt used for generation:
  

- Generated and saved image output as `futuristic_city.png`.

---

## 📸 Sample Output

(./futuristic_city.png)
C:\Users\a<img width="512" height="512" alt="futuristic_city (2)" src="https://github.com/user-attachments/assets/ecf2c2cc-a0f1-44db-9909-e51f2a8829f6" />
bhi\Downloads\futuristic_city (2).png

> *Image generated using Stable Diffusion based on the text prompt above.*

---

## 📌 Key Learnings

- Understanding latent diffusion techniques for image generation.
- Prompt engineering – how prompt detail impacts image fidelity.
- Comparing generation quality between DALL·E-mini and Stable Diffusion.

---

## 💡 How to Run the Code

### 🔗 [Google Colab Notebook](https://colab.research.google.com/drive/1f4X8hrYkroKXWjO7zWqbywSxPujz6eh1?usp=sharing)

```bash
# 1. Clone this repository or open the notebook
# 2. Run the cells step by step
# 3. Provide your Hugging Face token when prompted
# 4. Customize your own text prompts
