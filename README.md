# Abhishek-N
# ðŸ§  Task-02: Image Generation with Pre-trained Models

### ðŸ“ Internship at Prodigy Infotech  
**Domain:** AI / Machine Learning  
**Task:** Generate images from text prompts using pre-trained generative models.

---

## ðŸš€ Objective

To explore and implement **text-to-image generation** using cutting-edge **pre-trained models** like **Stable Diffusion** and **DALLÂ·E-mini**, transforming natural language descriptions into visual outputs.

---

## ðŸ§° Tools & Technologies

- ðŸ Python
- ðŸ¤— Hugging Face `diffusers`
- ðŸ”¥ PyTorch
- ðŸ§  Stable Diffusion v1.4 (`CompVis`)
- ðŸ–¼ï¸ DALLÂ·E-mini / Craiyon (optional)

---

## ðŸ§ª Experiment Setup

- Used Hugging Faceâ€™s `diffusers` to access `Stable Diffusion v1-4`.
- Loaded model to `cuda` or `cpu` based on availability.
- Prompt used for generation:
  

- Generated and saved image output as `futuristic_city.png`.

---

## ðŸ“¸ Sample Output

(./futuristic_city.png)

> *Image generated using Stable Diffusion based on the text prompt above.*

---

## ðŸ“Œ Key Learnings

- Understanding latent diffusion techniques for image generation.
- Prompt engineering â€“ how prompt detail impacts image fidelity.
- Comparing generation quality between DALLÂ·E-mini and Stable Diffusion.

---

## ðŸ’¡ How to Run the Code

### ðŸ”— [Google Colab Notebook](https://colab.research.google.com/drive/1f4X8hrYkroKXWjO7zWqbywSxPujz6eh1?usp=sharing)

```bash
# 1. Clone this repository or open the notebook
# 2. Run the cells step by step
# 3. Provide your Hugging Face token when prompted
# 4. Customize your own text prompts
